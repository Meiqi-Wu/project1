{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm # Instantly make loops show a smart progress meter\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read the Rating data\n",
    "ratings_df = pd.read_csv('./DATA/Ratings.csv').drop('Unnamed: 0', axis=1)\n",
    "ratings_df.review = ratings_df.review.astype(str)\n",
    "\n",
    "# divide reviews as negative and positive based on scores\n",
    "ratings_df['senti'] = ratings_df['score'].apply(lambda x: 1 if int(x)>2.5 else 0)\n",
    "# tokenize the data and vectorize the reviews\n",
    "max_features = 1000 # The maximu number of words to keep\n",
    "# create a tokenizer\n",
    "tokenizer = Tokenizer(num_words = max_features, split=' ')\n",
    "# fit the tokenizer on the review text\n",
    "tokenizer.fit_on_texts(ratings_df['review'].values)\n",
    "X = tokenizer.texts_to_sequences(ratings_df['review'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = pd.get_dummies(ratings_df['senti'].astype(int)).values\n",
    "y = ratings_df['senti'].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, embed_dim, lstm_dim, input_length):\n",
    "        super(net, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(max_features, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim * input_length, lstm_dim, dropout=0.2)\n",
    "        self.dense = nn.Linear(lstm_dim, 2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        embeds = self.word_embeddings(X)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(X), 1, -1))\n",
    "        out = F.softmax(self.dense(lstm_out.view(len(X), -1)), dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample = 10000 # X.shape[0]\n",
    "index = np.random.choice(X.shape[0], nsample, replace=False)\n",
    "X_train,X_test, y_train,y_test = train_test_split(X[index,:], y[index], random_state=0)\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "lstm_dim = 16\n",
    "\n",
    "model = net(embed_dim, lstm_dim, X_train.shape[1])\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    for idx in range(batch_size, len(X_train_torch)+1, batch_size):\n",
    "        X_train_batch = X_train_torch[idx-batch_size:idx, :]\n",
    "        y_train_batch = y_train_torch[idx-batch_size:idx]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        score = model(X_train_batch)\n",
    "        \n",
    "        loss = loss_function(score, y_train_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.72\n",
      "81.36\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_test == 1) / float(len(y_test)) * 100)\n",
    "print(np.sum(y_train == 1) / float(len(y_train)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.96"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "y_predict = np.where(model(X_test_torch)[:,1] > 0.5, 1, 0)\n",
    "np.sum(y_predict == y_test) / float(len(y_test)) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.52"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train accuracy\n",
    "n = 2500\n",
    "index = np.random.choice(X_train.shape[0], n, replace=False)\n",
    "y_train_predict = np.where(model(X_train_torch[index,:])[:,1] > 0.5, 1, 0)\n",
    "np.sum(y_train_predict == y_train[index]) / float(len(y_train[index]))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "torch.save(model.state_dict(), './model/senti_torch_embed{}_lstm{}.pth'.format(embed_dim, lstm_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
